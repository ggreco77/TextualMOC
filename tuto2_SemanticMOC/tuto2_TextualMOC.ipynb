{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0038b2-7e0f-426f-8cd2-5cd747d9b484",
   "metadata": {},
   "source": [
    "# Building and Handling Textual MOCs\n",
    "\n",
    "The notebook is associated to the submitted paper **Encapsulating Textual Contents into a MOC data structure for Advanced Applications**.\n",
    "The notebook outlines the basic functionalities of a new approach that integrates textual descriptions directly into the JSON representation of MOC, enabling simultaneous semantic and spatial operations. After demonstrating some basic applications and its potential use for educational gamification, we will later showcase its applicative capabilities in generative AI (GenAI).\n",
    "\n",
    "The tutorials are organized in the following folders\n",
    "1. [tuto1_TextualMOC](https://github.com/ggreco77/TextualMOC/tree/main/tuto1_TextualMOC) basic application to build a Textual MOC\n",
    "2. [AladinGame](https://github.com/ggreco77/TextualMOC/tree/main/AladinGame) using Text MOC for EDU game in Aladin Lite\n",
    "3. [tuto2_TextualMOC](https://github.com/ggreco77/TextualMOC/tree/main/tuto2_SemanticMOC) Creating Semantic MOC for application in Generative AI systems\n",
    "\n",
    "#### Version 0.0.7 - September 2025\n",
    "\n",
    "This notebook is divided into the following sections.\n",
    "\n",
    "1. [**Textual MOC Powered by GenAI**](#Textual-MOC-Powered-by-GenAI)\n",
    "\n",
    "   - [Semantic MOCs Generation and Management](#Semantic-MOCs-Generation-and-Management)  \n",
    "   - [RAG with textual MOC and Vision Models](#RAG-with-textual-MOC-and-Vision-Models)\n",
    "\n",
    " Basic Methods for Handling Textual MOCs\n",
    "\n",
    " \n",
    " Here are some basic applications of the **Textual MOC**, which enhances ordinary MOCs by encapsulating textual content. The `TextualMOC` class is designed to interact with a Multi-Order Coverage (MOC) object, enabling serialization, modification, and extension of MOC data with additional textual descriptions and image. The `__init__` method initializes the TextualMOC class with an optional MOC object. If a MOC object is provided, it is serialized into JSON format. Additionally, an `ipyaladin` widget is initialized for later use in visualizing the MOC.\n",
    "\n",
    "For using methods that transform textual content into semantic embeddings, we recommend installing and running Ollama - https://ollama.com/.\n",
    "\n",
    "**The complete list of methods is provided below**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15905d27-d2dc-4f53-99bd-8af02f88073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import requests\n",
    "#from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from ipyaladin import Aladin\n",
    "\n",
    "from mocpy import MOC\n",
    "import healpy as hp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import astropy.units as u\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4dc7fd-a5d3-4eab-8c59-48901ba63a07",
   "metadata": {},
   "source": [
    "**While we wait for an official library for textual MOCs,  we import some of the main methods required for the tutorial to work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca3b64-25dc-4dc3-a0fa-4f5739e0f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#While we wait for an official library for textual MOCs, \n",
    "# we import some of the main methods required for the tutorial to work.\n",
    "\n",
    "\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/ggreco77/TextualMOC/refs/heads/main/textualmoc/textual_moc.py\"\n",
    "dest = Path(\"textual_moc.py\")  # Change the path/name if you want\n",
    "\n",
    "if dest.exists():\n",
    "    print(f\"{dest} already exists; skipping download.\")\n",
    "else:\n",
    "    with requests.get(url, stream=True, timeout=30) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(dest, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    print(\"Saved to:\", dest.resolve())\n",
    "\n",
    "# importing TextualMOC class testing\n",
    "from textual_moc import TextualMOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d866bc08-ad80-4ef8-9611-46ee33a30ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Methods in TextualMOC ClassÂ¶\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "methods = [\n",
    "    {\n",
    "        \"Method\": \"add_text_media_image\",\n",
    "        \"Description\": \"Adds text, media and image to `TextualMOC` by reading from a file or a URL.\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"annotate_cell\",\n",
    "        \"Description\": \"Assigns a textual annotation to a specific MOC cell within the JSON data structure.\"\n",
    "        },\n",
    "    {\n",
    "        \"Method\": \"embedding_from_custom_text\",\n",
    "        \"Description\": \"Generates an embedding of the text using a specified service and model.\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"load_textual_moc\",\n",
    "        \"Description\": \"Loads an instance of `TextualMOC` from a JSON file.\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"plot_moc_area\",\n",
    "        \"Description\": \"Visualizes the MOC area using matplotlib.\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"show_image_value\",\n",
    "        \"Description\": \"Prints the image URL stored in the MOC data as a clickable link.\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"show_media_value\",\n",
    "        \"Description\": \"Prints the multimedia URL stored in the MOC data.\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"show_metadata_value\",\n",
    "        \"Description\": \"Prints metadata information such as author, date, and last text update.\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"show_text_value\",\n",
    "        \"Description\": \"Prints the custom text stored in the MOC data.\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"render\",\n",
    "        \"Description\": \"Loads the MOC from a JSON file and displays text, media, MOC area, metadata, image and embedding if present.\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"render_ipyaladin\",\n",
    "        \"Description\": \"Displays the MOC in an Aladin viewer with defined colors, transparency, and HiPS.\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"save\",\n",
    "        \"Description\": \"Saves the current state of `TextualMOC` in JSON format.\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"update_metadata\",\n",
    "        \"Description\": \"Updates metadata such as author and date in the MOC's JSON data.\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"update_text_inline\",\n",
    "        \"Description\": \"Appends new text to the custom text stored in the MOC's JSON data.\"\n",
    "    },\n",
    "#    {\n",
    "#        \"Method\": \"union\",\n",
    "#        \"Description\": \"Merges the current MOC instance with another instance of `TextualMOC`.\"\n",
    "#    }\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "df_methods = pd.DataFrame(methods)\n",
    "\n",
    "# Sort the DataFrame alphabetically by the method name\n",
    "df_methods = df_methods.sort_values(by=\"Method\").reset_index(drop=True)\n",
    "\n",
    "# Adjust the index to start at 1 instead of 0\n",
    "df_methods.index = df_methods.index + 1\n",
    "df_methods.index.name = 'No.'\n",
    "\n",
    "# Prevent pandas from truncating the descriptions\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Define the title\n",
    "title = \"# List of Methods in `TextualMOC`\"\n",
    "\n",
    "# Display the title and the table\n",
    "display(Markdown(title))\n",
    "display(df_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493c6cc-aa9f-486a-9246-111b02bd2af8",
   "metadata": {},
   "source": [
    "# Textual MOC Powered by GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9236ff9-8db0-477e-bd3d-c5e0e65a877b",
   "metadata": {},
   "source": [
    "## Semantic MOCs Generation and Management\n",
    "\n",
    "In the previous sections, we introduced Textual MOCs and developed some examples of how they can be applied. Now we proceed to semantic MOCs, where the textual content is transformed into embeddings. This involves converting the associated text of each MOC into numerical vectors using machine learning models, enabling advanced analysis and comparison.\n",
    "\n",
    "The `embedding_from_custom_text` method of  `TextualMOC` class is used to generate numerical representations (embeddings) of the custom text stored in a textual MOC, allowing it to capture semantic meaning in a multidimensional space for easier analysis and comparison. The function first checks whether the MOC contains the `custom_text` key; if present, it extracts the text for processing; otherwise, it prints an informative message. Next, an embedding model is instantiated using [OllamaEmbeddings](https://python.langchain.com/docs/integrations/text_embedding/ollama/) from [LangChain](https://www.langchain.com/), with the default model set to `nomic-embed-text`, which can be modified by the user via a parameter. Once the model is selected, it generates a numerical vector representation of the text that is then stored in the MOC under the `embedding` key.  Additionally, the name of the model used is saved in the `model` key to ensure traceability. \n",
    "\n",
    "For using that method, we recommend installing and running Ollama - https://ollama.com/.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171dd4aa-70ef-4bdd-9fbc-c55b5c85295f",
   "metadata": {},
   "source": [
    "#### From Textual MOC to Semantic MOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d6e18-c1c9-49b4-b25f-85d51545f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textual MOC path\n",
    "file_path = \"textual_moc_example.json\"\n",
    "\n",
    "# Instantiate TextualMOC\n",
    "textual_moc = TextualMOC()\n",
    "\n",
    "# Loading text content form the Textual MOC\n",
    "textual_moc.load_textual_moc(file_path)  \n",
    "\n",
    "# Generating an embedding using a specified service and model; here  \"nomic-embed-text\" is used\n",
    "textual_moc.embedding_from_custom_text(embeddings_model=\"nomic-embed-text-v2-moe\")\n",
    "\n",
    "# Check if the embedding was added correctly\n",
    "print(\"Embedding present?\", \"embedding\" in textual_moc.moc_data)\n",
    "print(\"Model used:\", textual_moc.moc_data.get(\"embedding_model\", \"No model\"))\n",
    "print(\"Embedding size:\", len(textual_moc.moc_data.get(\"embedding\", [])))\n",
    "\n",
    "# Save the Semantic MOC in a file\n",
    "textual_moc.save(\"moc_data_with_embedding.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb7edc-9812-47eb-86fd-aa7382a7fa41",
   "metadata": {},
   "source": [
    "#### Managing a Semantic MOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39810e42-2d6b-4c01-b840-653d1e4fb776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic MOC path - textual MOC with semantic embedding\n",
    "file_path = \"moc_data_with_embedding.json\"\n",
    "\n",
    "# Create a TextualMOC instance\n",
    "textual_moc = TextualMOC()\n",
    "\n",
    "# Load the local MOC \n",
    "textual_moc.load_textual_moc(file_path)\n",
    "\n",
    "# Check if the embedding is present\n",
    "if \"embedding\" in textual_moc.moc_data:\n",
    "    embedding = textual_moc.moc_data[\"embedding\"]\n",
    "    embedding_model = textual_moc.moc_data.get(\"model\", \"Model not specified\")\n",
    "\n",
    "    print(\"Embedding is present in the MOC.\")\n",
    "    print(f\"Embedding model used: {embedding_model}\")\n",
    "    print(f\"Embedding size: {len(embedding)}\")\n",
    "\n",
    "    # If you want to print a portion of the embedding (e.g., the first 5 values):\n",
    "    print(\"First 5 dimensions of the embedding:\", embedding[:5])\n",
    "else:\n",
    "    print(\"No embedding found in the MOC.\")\n",
    "\n",
    "# Print other metadata for verification\n",
    "textual_moc.show_metadata_value()  \n",
    "textual_moc.show_text_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dd689e-49cf-4f3d-b480-c45e5be58388",
   "metadata": {},
   "source": [
    "# Vector Databases, RAG and Visual Models\n",
    "\n",
    "### Data generation for GenAI applications\n",
    "\n",
    "Here, we generate a set of spatial MOCs corresponding to the coordinates of astronomical objects of interest. As an illustrative example, we consider well-known spiral, elliptical, and irregular galaxies, some of which exhibit clear evidence of tidal interactions. The process for creating spatial MOCs is performed using SIMBAD, which provides the coordinates of each object; subsequently, circular MOCs are generated around these coordinates, adopting a predefined radius.\n",
    "\n",
    "In addition, we generate a set of textual MOCs that describe each astronomical object, incorporating an \"image\" key. The corresponding images are retrieved from the hips2fits server. Specifically, in this case, the HiPS2FITS service utilizes an HTTP-based web API, where parameters are directly passed within the URL as query string parameters.\n",
    "\n",
    "These textual/semantic MOCs, each containing a reference to a HiPS image, are stored in a directory for subsequent processing by generative AI models, both textual and visual, as previously described.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f18d9-bef0-42ad-a29b-8d296dcc122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from astroquery.simbad import Simbad\n",
    "from astropy.coordinates import SkyCoord, Longitude, Latitude, Angle\n",
    "import astropy.units as u\n",
    "from mocpy import MOC\n",
    "\n",
    "# Object galaxy list\n",
    "galaxies = [\"Arp273\", \"M59\", \"NGC4676\", \"M101\", \"M60\", \"NGC4993\", \n",
    "            \"M104\", \"M82\", \"NGC4038\", \"M51\", \"M87\"]\n",
    "\n",
    "# Building Space MOC at the galaxy position\n",
    "for galaxy in galaxies:\n",
    "    result = Simbad.query_object(galaxy)\n",
    "\n",
    "    if result is not None:\n",
    "        # Extracting RA/DEC\n",
    "        ra = result[\"RA\"][0]  \n",
    "        dec = result[\"DEC\"][0]  \n",
    "        coords = SkyCoord(ra, dec, unit=(u.hourangle, u.deg))\n",
    "\n",
    "        print(f\"Coordinates \"+ galaxy)\n",
    "        print(f\"RA  (Right Ascension) : {coords.ra.deg}Â°\")\n",
    "        print(f\"DEC (Declination)      : {coords.dec.deg}Â°\")\n",
    "\n",
    "        # --- Step 2: Creating circle MOC with radius = 0.1Â° ---\n",
    "        radius = Angle(0.1, u.deg)  # fixed radius\n",
    "        moc = MOC.from_cone(\n",
    "            lon=Longitude(coords.ra),\n",
    "            lat=Latitude(coords.dec),\n",
    "            radius=radius,\n",
    "            max_depth=14  # MOC resolution\n",
    "        )\n",
    "\n",
    "        # --- Step 3: Creating moc_gals dir ---\n",
    "        save_dir = \"moc_gals\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # --- Step 4: Saving MOC in json format ---\n",
    "        moc_json = moc.serialize(format='json')\n",
    "        save_path = os.path.join(save_dir, galaxy+\".json\")\n",
    "\n",
    "        with open(save_path, \"w\") as json_file:\n",
    "            json.dump(moc_json, json_file, indent=4)\n",
    "\n",
    "        print(f\"Space MOC saved in  {save_path}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No object in Simbad.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e7b22-80e6-4072-940a-1c5ca1bca443",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Getting image from hips2fits - https://alasky.cds.unistra.fr/hips-image-services/hips2fits\n",
    "Arp273_ima = \"https://alasky.cds.unistra.fr/hips-image-services/hips2fits?hips=CDS%2FP%2FDSS2%2Fcolor&width=1200&height=900&fov=0.1&projection=SIN&coordsys=icrs&rotation_angle=0.0&object=Arp%20273&format=jpg\"\n",
    "NGC4676_ima= \"https://alasky.cds.unistra.fr/hips-image-services/hips2fits?hips=CDS%2FP%2FDSS2%2Fcolor&width=1200&height=900&fov=0.1&projection=SIN&coordsys=icrs&rotation_angle=0.0&object=NGC%204676&format=jpg\"\n",
    "NGC4038_ima =\"https://alasky.cds.unistra.fr/hips-image-services/hips2fits?hips=CDS%2FP%2FHST%2FEPO&width=1000&height=1000&fov=0.1&projection=SIN&coordsys=icrs&rotation_angle=0.0&ra=180.4790760656&dec=-18.884864677&format=jpg\"\n",
    "M51_ima = \"https://alasky.cds.unistra.fr/hips-image-services/hips2fits?hips=CDS%2FP%2FDSS2%2Fcolor&width=1200&height=900&fov=0.3&projection=SIN&coordsys=icrs&rotation_angle=0.0&object=M51&format=jpg\"\n",
    "M101_ima = \"https://alasky.cds.unistra.fr/hips-image-services/hips2fits?hips=CDS%2FP%2FDSS2%2Fcolor&width=1200&height=900&fov=0.35&projection=SIN&coordsys=icrs&rotation_angle=0.0&object=M101&format=jpg\"\n",
    "M104_ima = \"https://alasky.cds.unistra.fr/hips-image-services/hips2fits?hips=CDS%2FP%2FDSS2%2Fcolor&width=1200&height=900&fov=0.2&projection=SIN&coordsys=icrs&rotation_angle=0.0&object=M104&format=jpg\"\n",
    "M87_ima = \"https://alasky.cds.unistra.fr/hips-image-services/hips2fits?hips=CDS%2FP%2FDSS2%2Fcolor&width=1200&height=900&fov=0.15&projection=SIN&coordsys=icrs&rotation_angle=0.0&object=M87&format=jpg\"\n",
    "M59_ima = \"https://alasky.cds.unistra.fr/hips-image-services/hips2fits?hips=CDS%2FP%2FDSS2%2Fcolor&width=1200&height=900&fov=0.1&projection=SIN&coordsys=icrs&rotation_angle=0.0&object=M59&format=jpg\"\n",
    "M60_ima = \"https://alasky.cds.unistra.fr/hips-image-services/hips2fits?hips=CDS%2FP%2FDSS2%2Fcolor&width=1200&height=900&fov=0.15&projection=SIN&coordsys=icrs&rotation_angle=0.0&object=M60&format=jpg\"\n",
    "NGC4993_ima = \"https://alasky.cds.unistra.fr/hips-image-services/hips2fits?hips=CDS%2FP%2FDSS2%2Fcolor&width=1200&height=900&fov=0.04&projection=SIN&coordsys=icrs&rotation_angle=0.0&object=NGC%204993&format=jpg\"\n",
    "M82_ima = \"https://alasky.cds.unistra.fr/hips-image-services/hips2fits?hips=CDS%2FP%2FDSS2%2Fcolor&width=1200&height=900&fov=0.2&projection=SIN&coordsys=icrs&rotation_angle=0.0&object=M82&format=jpg\"\n",
    "#NGC4449_ima = \"https://alasky.cds.unistra.fr/hips-image-services/hips2fits?hips=CDS%2FP%2FDSS2%2Fcolor&width=1200&height=900&fov=0.15&projection=SIN&coordsys=icrs&rotation_angle=0.0&object=NGC%204449&format=jpg\"\n",
    "\n",
    "# Short description text\n",
    "Arp273_text = \"The galaxies' twisted and distorted appearance is due to mutual gravitational tides as the pair engage in \\\n",
    "close encounters. Cataloged as Arp 273 (also as UGC 1810), these galaxies do look peculiar, but interacting galaxies are now \\\n",
    "understood to be common in the universe. Closer to home, the large spiral Andromeda Galaxy is known to be some 2 million light-years away \\\n",
    "and inexorably approaching the Milky Way. In fact the far away peculiar galaxies of Arp 273 may offer an analog of the far future \\\n",
    "encounter of Andromeda and Milky Way. Repeated galaxy encounters on a cosmic timescale ultimately result in a merger into a \\\n",
    "single galaxy of stars. From our perspective, the bright cores of the Arp 273 galaxies are separated by only a little over 100,000 light-years. \"\n",
    "\n",
    "NGC4676_text= \"This colliding pair of spiral galaxies is known as 'The Mice' because of the long tails of stars and gas emanating \\\n",
    "from each galaxy. Otherwise known as NGC 4676, they will eventually merge into a single giant galaxy. In the galaxy at left, \\\n",
    "the bright blue patch can be identified as a cascade of clusters and associations of young, hot blue stars, whose \\\n",
    "formation has been triggered by the tidal forces of the gravitational interaction. Streams of material can also be seen \\\n",
    "flowing between the two galaxies in this Hubble Space Telescope image. \\\n",
    "The clumps of young stars in the long, straight tidal tail (upper right) are separated by fainter regions \\\n",
    "of material. These dim regions suggest that the clumps of stars have formed from the gravitational \\\n",
    "collapse of the gas and dust that once occupied those areas. Some of the clumps have luminous masses \\\n",
    "comparable to dwarf galaxies that orbit in the halo of our own Milky Way.\"\n",
    "\n",
    "NGC4038_text = \"This new NASA Hubble Space Telescope image of the Antennae galaxies is the sharpest yet \\\n",
    "of this merging pair of galaxies. During the course of the collision, billions of stars will be formed. The brightest \\\n",
    "and most compact of these star birth regions are called super star clusters. The two spiral galaxies started to interact \\\n",
    "a few hundred million years ago, making the Antennae galaxies one of the nearest and youngest examples of a pair of colliding galaxies. \\\n",
    "Nearly half of the faint objects in the Antennae image are young clusters containing tens of thousands of stars. \\\n",
    "The orange blobs to the left and right of image center are the two cores of the original galaxies and consist mainly \\\n",
    "of old stars criss-crossed by filaments of dust, which appears brown in the image. The two galaxies are dotted with brilliant \\\n",
    "blue star-forming regions surrounded by glowing hydrogen gas, appearing in the image in pink.\"\n",
    "\n",
    "M51_text = \"M 51, an interacting spiral galaxy, which is also known as the Whirlpool Galaxy. It is located about 25 Million light \\\n",
    "years away from Earth, but can still easily observed with a small telescope by amateur astronomers. M 51 is also a popular object among \\\n",
    "professional astronomers as it shows an ongoing enhanced star formation rate, which is probably caused by the interaction with its \\\n",
    "companion galaxy. The galaxy was also the location of two supernovae within the last couple of years: \\\n",
    "The first one appeared in 2005, the second one in 2011.\"\n",
    "\n",
    "M101_text = \"Messier 101 is a classic, face-on, pinwheel spiral galaxy. The giant spiral disk of stars, dust and gas is 170,000 light-years across â nearly \\\n",
    "twice the diameter of our galaxy, the Milky Way. M101 is estimated to contain at least one trillion stars. The galaxyâs spiral arms are \\\n",
    "sprinkled with large regions of star-forming nebulas. These nebulas are areas of intense star formation within giant molecular \\\n",
    "hydrogen clouds. Brilliant, young clusters of hot, blue, newborn stars trace out the spiral arms. Pierre MÃ©chain, one of \\\n",
    "Charles Messierâs colleagues, discovered the Pinwheel galaxy in 1781. Located 25 million light-years away from Earth in \\\n",
    "the constellation Ursa Major, M101 has an apparent magnitude of 7.9. It can be spotted through a small telescope and is \\\n",
    "most easily observed during June.\"\n",
    "\n",
    "M104_text = \"One of most famous spiral galaxies is Messier 104, widely known as the 'Sombrero' (the Mexican hat) because of \\\n",
    "its particular shape. It is located towards the constellation Virgo, at a distance of about 30 million light-years \\\n",
    "and is the 104th object in the famous catalogue of deep-sky objects by French astronomer Charles Messier (1730 - 1817).\\\n",
    "This luminous and massive galaxy has a total mass of about 800 billion suns, and is notable for its dominant nuclear bulge,\\\n",
    "composed mainly of mature stars, and its nearly edge-on disc composed of stars, gas, and dust. The complexity of this dust \\\n",
    "is apparent directly in front of the bright nucleus, but is also evident in the dark absorbing lanes throughout the disc. \\\n",
    "A large number of small, diffuse objects can be seen as a swarm in the halo of Messier 104. Most of these are globular clusters,\\\n",
    "similar to those found in our own Milky Way, but Messier 104 has a much larger number of them.\"\n",
    "\n",
    "M87_text = \"The elliptical galaxy M87 is the home of several trillion stars, a supermassive black hole and a family of roughly\\\n",
    "15,000 globular star clusters. For comparison, our Milky Way galaxy contains only a few hundred billion stars and about 150 globular \\\n",
    "clusters. The monstrous M87 is the dominant member of the neighboring Virgo cluster of galaxies, which contains some 2,000 galaxies. \\\n",
    "Discovered in 1781 by Charles Messier, this galaxy is located 54 million light-years away from Earth in the constellation Virgo. \\\n",
    "It has an apparent magnitude of 9.6 and can be observed using a small telescope most easily in May.\"\n",
    "\n",
    "M59_text = \"M59 is one of the largest elliptical galaxies in the Virgo galaxy cluster. However, it is still considerably less massive, \\\n",
    "and at a magnitude of 9.8, less luminous than other elliptical galaxies in the cluster. A supermassive black hole around 270 million times \\\n",
    "as massive as the Sun resides at the center of M59. The galaxy also has an inner disk of stars and around 2,200 globular clusters, \\\n",
    "an exceptionally high number of such clusters. The central region of the galaxy, the inner 200 light-years, rotates in the opposite \\\n",
    "direction than the rest of the galaxy and is the smallest region in a galaxy known to exhibit this behavior. \\\n",
    "Approximately 60 million light-years from Earth, M59 can be found near M58 and M60 in the constellation Virgo. It is best seen in May. \\\n",
    "Small telescopes might reveal an ellipsoidal shape with a bright center, but even larger scopes do not reveal much detail. \\\n",
    "German astronomer Johann Gottfried Koehler discovered M59 and the nearby galaxy M60 in the spring of 1779 when observing the comet \\\n",
    "of that year (Comet Bode).\"\n",
    "\n",
    "M60_text = \"The Virgo cluster is a collection of more than 1,300 galaxies, including the elliptical galaxy M60. Unlike spiral galaxies, \\\n",
    "elliptical galaxies lack an organized structure and are nearly featureless, resembling the core of a spiral galaxy. \\\n",
    "The Virgo clusterâs third brightest member, M60 has a diameter of 120,000 light-years and is as massive as one trillion suns. \\\n",
    "At its center lies a huge black hole, 4.5 billion times as massive as the sun â one of the most massive black holes ever found. \\\n",
    "NGC 4647 is about two-thirds the size of M60 â or roughly the size of the Milky Way galaxy â and is much less massive. \\\n",
    "The two galaxies form a pair known as Arp 116. Astronomers have long tried to determine whether these two galaxies are actually interacting. \\\n",
    "Although from Earth they appear to overlap,there is no evidence of new star formation, which would be one of the clearest signs that \\\n",
    "the two galaxies are indeed interacting. However, recent studies of very detailed Hubble images suggest the onset of some tidal \\\n",
    "interaction between the two.\"\n",
    "\n",
    "NGC4993_text = \"The elliptical galaxy NGC 4993, about 130 million light-years from Earth, viewed with the VIMOS instrument on the European Southern Observatory's Very Large Telescope \\\n",
    "in Chile.After the almost simultaneous detection of gravitational waves by the LIGO/Virgo collaboration, GW170817, and of a gamma-ray burst \\\n",
    "by ESA's INTEGRAL and NASA's Fermi satellites, GRB170817, a large number of ground and space telescopes started searching for the source in the sky. \\\n",
    "About half a day later, scientists at various optical observatories spotted something new near the core of galaxy NGC 4993: \\\n",
    "this was the visible light counterpart to the gravitational waves and the gamma-ray burst, confirming that they originated \\\n",
    "from the collision of two neutron stars. The result of such a cosmic clash is a kilonova: the neutron-rich material released \\\n",
    "in the merger is impacting its surroundings, forging a wealth of heavy elements in the process. The kilonova can be seen just above \\\n",
    "and slightly to the left of the centre of the galaxy, AT2017gfo.\"\n",
    "\n",
    "M82_text = \"Located 12 million light-years away, M82 appears high in the northern spring sky in the direction of the constellation \\\n",
    "Ursa Major, the Great Bear. It is also called the 'Cigar Galaxy' because of the elongated elliptical shape produced by \\\n",
    "the tilt of its starry disk relative to our line of sight. As shown in this mosaic image, M82 is a magnificent starburst galaxy. \\\n",
    "Throughout its central region young stars are being born ten times faster than they are inside in our Milky Way Galaxy.\\\n",
    "These numerous hot new stars not only emit radiation but also charged particles that form the so-called stellar wind. \\\n",
    "Stellar winds streaming from these stars combine to form a galactic 'superwind'.\"\n",
    "\n",
    "#NGC4449_text = \"\"\n",
    "\n",
    "# Multimedia URL from text is provided - part of the text has been adapted.\n",
    "Arp273_media = \"https://apod.nasa.gov/apod/ap250109.html\"\n",
    "NGC4676_media= \"https://science.nasa.gov/image-detail/idl-tiff-file-40/\"\n",
    "NGC4038_media =\"https://hubblesite.org/contents/media/images/2006/46/1995-Image\"\n",
    "M51_media = \"https://esahubble.org/images/opo0521b/\"\n",
    "M101_media = \"https://science.nasa.gov/mission/hubble/science/explore-the-night-sky/hubble-messier-catalog/messier-101/\"\n",
    "M104_media = \"https://www.eso.org/public/images/sombrero/\"\n",
    "M87_media = \"https://science.nasa.gov/mission/hubble/science/explore-the-night-sky/hubble-messier-catalog/messier-87/#:~:text=The%20elliptical%20galaxy%20M87%20is,and%20about%20150%20globular%20clusters.\"\n",
    "M59_media = \"https://science.nasa.gov/mission/hubble/science/explore-the-night-sky/hubble-messier-catalog/messier-59/\"\n",
    "M60_media = \"https://science.nasa.gov/mission/hubble/science/explore-the-night-sky/hubble-messier-catalog/messier-60/\"\n",
    "NGC4993_media = \"https://sci.esa.int/web/integral/-/59671-new-source-in-galaxy-ngc-4993\"\n",
    "M82_media = \"https://www.esa.int/Science_Exploration/Space_Science/Hubble_s_view_of_Cigar_Galaxy_on_sixteenth_mission_anniversary\"\n",
    "#NGC4449_media = \"\"\n",
    "\n",
    "# Text description list\n",
    "text_files = [Arp273_text, M59_text, NGC4676_text, M101_text, M60_text, NGC4993_text, \n",
    "              M104_text,M82_text, NGC4038_text, M51_text, M87_text]\n",
    "\n",
    "# Multimedia link list\n",
    "multimedia_urls = [Arp273_media, M59_media, NGC4676_media, M101_media, M60_media, NGC4993_media, \n",
    "                   M104_media,M82_media, NGC4038_media, M51_media, M87_media]\n",
    "\n",
    "# Images fron hips2fits list\n",
    "images = [Arp273_ima, M59_ima, NGC4676_ima, M101_ima, M60_ima, NGC4993_ima, \n",
    "          M104_ima,M82_ima,\tNGC4038_ima, M51_ima, M87_ima]\n",
    "\n",
    "# Space MOCs list \n",
    "text_moc_gals = [\"Arp273.json\", \"M59.json\", \"NGC4676.json\", \"M101.json\", \"M60.json\", \"NGC4993.json\", \n",
    "                 \"M104.json\",\"M82.json\", \"NGC4038.json\", \"M51.json\", \"M87.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d735d1a4-fad8-4e2d-82ed-334ca08b2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Textual MOCs generation\n",
    "\n",
    "mocs_directory = Path(\"moc_gals\")  # Directory in which the Textual MOCs are loaded\n",
    "\n",
    "for text_moc_gal, text_file, multimedia_url, image in zip(text_moc_gals, \n",
    "                                                          text_files, multimedia_urls, images):\n",
    "    file_path = os.path.join(mocs_directory, text_moc_gal)  # path\n",
    "    \n",
    "    # ð¹ 1. Loading MOCs\n",
    "    textual_moc.load_textual_moc(file_path)\n",
    "    \n",
    "    # ð¹ 2. Adding text, media, image URL\n",
    "    textual_moc.add_text_media_image(text_file, multimedia_url, \n",
    "                                     image)\n",
    "    \n",
    "     # ð¹ 3. Saving\n",
    "    textual_moc.save(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359d1ee7-7005-444a-8232-6e1074c11e4e",
   "metadata": {},
   "source": [
    "## RAG with textual MOC and Vision Models\n",
    "\n",
    "This section summarizes the main components of the RAG + MOC pipeline implementation:\n",
    "\n",
    "1. **Reading and Preparing MOCs**  \n",
    "   - `read_mocs_from_directory(directory: str) â List[str]`  \n",
    "     Scans a folder and returns all `.json` files containing Textual MOC data.  \n",
    "   - `process_mocs(moc_files: List[str], model: SentenceTransformer) â (metadata, embeddings, filenames)`  \n",
    "     Loads each JSON, extracts the `custom_text` field, and computes a normalized embedding via a Sentence-BERT model.  \n",
    "\n",
    "2. **Building the Vector Index**  \n",
    "   - `build_faiss_index(embeddings: np.ndarray) â faiss.IndexFlatIP`  \n",
    "     Creates a FAISS inner-product index over the normalized embeddings for efficient cosine-similarity search.  \n",
    "\n",
    "3. **Querying the Index**  \n",
    "   - `query_mocs(query_text: str, index, model, metadata, filenames, top_k=5) â List[(moc, score, filename)]`  \n",
    "     Encodes the userâs query, searches the FAISS index for the top K nearest embeddings, and returns matching MOC metadata.  \n",
    "\n",
    "4. **Generating a RAG Response**  \n",
    "   - `generate_response_with_mistral(query: str, contexts: List[(int, str)]) â str`  \n",
    "     Builds a system+user prompt containing the top-k text fragments and invokes a Mistral LLM (via LangChain/Ollama) to produce a grounded answer.  \n",
    "\n",
    "5. **Spatial Visualization**  \n",
    "   - `visualize_moc_in_aladin(moc, similarity: float, filename: str, survey: str, fov: int)`  \n",
    "     Launches an Aladin Lite widget, overlays the selected MOC footprint on an all-sky image, and displays metadata.  \n",
    "\n",
    "6. **Vision-LLM Analysis**  \n",
    "   - `analyze_image_with_vision_llm(image_url: str, vision_llm, prompt: str) â str`  \n",
    "     Fetches and base64-encodes the FITS image, then calls a vision-enabled LLM to extract morphological insights.  \n",
    "\n",
    "7. **Main Pipeline (`main()`)**  \n",
    "   Orchestrates the end-to-end flow:  \n",
    "   - Loads MOCs and builds the index  \n",
    "   - Runs the RAG retrieval and text generation  \n",
    "   - Visualizes the best match in Aladin  \n",
    "   - Invokes the vision LLM for additional image analysis  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a80b73-af5d-4c50-8028-b464741a1376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a99f50b-4918-4507-939f-dfd096fd054e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RAG + MOC visualizer â dual-mode with spatial debug\n",
    "===================================================\n",
    "\n",
    "- Positional mode: test (RA, DEC) â MOC using Angle-based contains_lonlat(), with clear debug prints.\n",
    "- Semantic mode: FAISS retrieval on custom_text + strict USED: line.\n",
    "- ipyaladin overlay of USED MOCs.\n",
    "- Vision LLM on each USED MOC image (download â base64 data-URI).\n",
    "\n",
    "ENV toggles (optional):\n",
    "  TEXT_MODEL, TEXT_TEMPERATURE\n",
    "  VISION_MODEL, VISION_TEMPERATURE\n",
    "  EMBEDDING_MODEL\n",
    "  ALADIN_SURVEY, ALADIN_FOV\n",
    "  SIMILARITY_THRESHOLD\n",
    "  MOC_DIR\n",
    "  DEBUG=1                     # extra verbose prints\n",
    "  ADD_SYNTHETIC_MOC=1         # append a synthetic cone MOC around the chosen RA/DEC (positional tests)\n",
    "\n",
    "*** Your TextualMOC class must be defined/imported in this session. ***\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss  # type: ignore\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# -----------------------------\n",
    "# âï¸ Config / Constants\n",
    "# -----------------------------\n",
    "TEXT_MODEL_NAME: str = os.environ.get(\"TEXT_MODEL\", \"mistral\")\n",
    "TEXT_TEMPERATURE: float = float(os.environ.get(\"TEXT_TEMPERATURE\", \"0\"))\n",
    "VISION_MODEL_NAME: str = os.environ.get(\"VISION_MODEL\", \"gemma3:4b\")\n",
    "VISION_TEMPERATURE: float = float(os.environ.get(\"VISION_TEMPERATURE\", \"0\"))\n",
    "\n",
    "EMBEDDING_MODEL_NAME: str = os.environ.get(\"EMBEDDING_MODEL\", \"paraphrase-MiniLM-L3-v2\")\n",
    "\n",
    "DEFAULT_SURVEY: str = os.environ.get(\"ALADIN_SURVEY\", \"CDS/P/DSS2/color\")\n",
    "DEFAULT_FOV: int = int(os.environ.get(\"ALADIN_FOV\", \"180\"))\n",
    "\n",
    "SIMILARITY_THRESHOLD: float = float(os.environ.get(\"SIMILARITY_THRESHOLD\", \"0.4\"))\n",
    "\n",
    "DEBUG: bool = os.environ.get(\"DEBUG\", \"0\") == \"1\"\n",
    "ADD_SYNTHETIC_MOC: bool = os.environ.get(\"ADD_SYNTHETIC_MOC\", \"0\") == \"1\"\n",
    "\n",
    "# Default visual task/context\n",
    "VISION_TASK_DEFAULT: str = os.environ.get(\n",
    "    \"VISION_TASK\",\n",
    "    \"Classify the galaxy as one of: spiral | elliptical | irregular | I don't know. \"\n",
    "    \"Also add a brief note about any visible interactions with other galaxies \"\n",
    "    \"(e.g., tidal tails, bridges, distortions, close companions).\"\n",
    ")\n",
    "VISION_CONTEXT_DEFAULT: str = os.environ.get(\n",
    "    \"VISION_CONTEXT\",\n",
    "    \"If the image is ambiguous, too small, or low quality, answer exactly: I don't know.\"\n",
    ")\n",
    "\n",
    "#IMAGE_ANALYZER_PROMPT: str = (\n",
    "#    \"Task: {task}\\n\"\n",
    "#    \"Context: {context}\\n\"\n",
    "#    \"Instructions:\\n\"\n",
    "#    \"- If you are uncertain, answer exactly: I don't know.\\n\"\n",
    "#    \"- Choose <class> from: spiral | elliptical | irregular | I don't know.\\n\"\n",
    "#    \"- Always include a brief morphology note (<= 7 words) based only on visible cues.\\n\"\n",
    "#    \"- If interactions are visible, briefly describe them (<= 1 short sentence). \"\n",
    "#    \"If not assessable, write: Interactions: I don't know.\\n\"\n",
    "#    \"Output format: <class>. Morphology: <brief note>. Interactions: <brief note>.\"\n",
    "#)\n",
    "\n",
    "IMAGE_ANALYZER_PROMPT: str = (\n",
    "    \"Task: {task}\\n\"\n",
    "    \"Context: {context}\\n\"\n",
    "    \"Instructions:\\n\"\n",
    "    \"- Predict <class> from: spiral | elliptical | irregular | I don't know.\\n\"\n",
    "    \"- Provide a confidence score in percent (0â100).\\n\"\n",
    "    \"- Give a brief morphology note (<= 20 words), based only on visible cues.\\n\"\n",
    "    \"- Describe tidal/interactions if visible; otherwise state what prevents assessment.\\n\"\n",
    "    \"- Add one short caption (<= 40 words) summarizing the scene.\\n\"\n",
    "    \"Output format:\\n\"\n",
    "    \"<class> (confidence=<0-100>%).\\n\"\n",
    "    \"Morphology: <note>.\\n\"\n",
    "    \"Interactions: <note>.\\n\"\n",
    "    \"Caption: <one sentence>.\"\n",
    ")\n",
    "\n",
    "TEXTUAL_SYSTEM_PROMPT = \"\"\"\n",
    "You are an astrophysics assistant.\n",
    "Use ONLY the provided excerpts to answer, clearly and concisely.\n",
    "Cite evidence with inline tags like [Doc n].\n",
    "If the information is insufficient, answer exactly: I don't know.\n",
    "\"\"\".strip()\n",
    "\n",
    "OUTPUT_RULES_SUFFIX = \"\"\"\n",
    "Output rules:\n",
    "- Use only the necessary documents; if they are not enough, answer exactly: I don't know.\n",
    "- Add inline citations such as [Doc 1], [Doc 2] next to claims.\n",
    "- If you cannot cite at least one document, DO NOT provide any explanation.\n",
    "  In that case, write ONLY the final line: USED:\n",
    "- Always END with EXACTLY one line in this format:\n",
    "  USED: 1,2   (numbers separated by commas; if none, leave it blank after the colon)\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# Types\n",
    "ResultTuple = Tuple[object, float, str]  # (moc_obj, similarity, filename)\n",
    "\n",
    "\n",
    "def print_runtime_config(directory: str) -> None:\n",
    "    print(\"=== CONFIG RUNTIME ===\")\n",
    "    print(f\"Embedding model       : {EMBEDDING_MODEL_NAME}\")\n",
    "    print(f\"Text model            : {TEXT_MODEL_NAME} (T={TEXT_TEMPERATURE})\")\n",
    "    print(f\"Vision model          : {VISION_MODEL_NAME} (T={VISION_TEMPERATURE})\")\n",
    "    print(f\"Aladin                : survey={DEFAULT_SURVEY}, fov={DEFAULT_FOV}\")\n",
    "    print(f\"MOC directory         : {directory}\")\n",
    "    print(f\"Similarity threshold  : {SIMILARITY_THRESHOLD}\")\n",
    "    print(f\"DEBUG                 : {DEBUG}\")\n",
    "    print(f\"ADD_SYNTHETIC_MOC     : {ADD_SYNTHETIC_MOC}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers (no changes to TextualMOC)\n",
    "# -----------------------------\n",
    "def read_mocs_from_directory(directory: str) -> List[str]:\n",
    "    try:\n",
    "        return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".json\")]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory not found: {directory}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def _ensure_textual_moc_available() -> None:\n",
    "    if \"TextualMOC\" not in globals() or not isinstance(TextualMOC, type):\n",
    "        raise ImportError(\n",
    "            \"TextualMOC is not available. Define/import it in this session before running.\\n\"\n",
    "            \"It must expose: .load_textual_moc(path), .moc_data (dict), and .moc (mocpy.MOC).\"\n",
    "        )\n",
    "\n",
    "\n",
    "def load_all_mocs(moc_files: List[str]) -> Tuple[List[object], List[str]]:\n",
    "    _ensure_textual_moc_available()\n",
    "    TM = TextualMOC\n",
    "\n",
    "    metadata: List[object] = []\n",
    "    filenames: List[str] = []\n",
    "\n",
    "    for file_path in moc_files:\n",
    "        m = TM()\n",
    "        try:\n",
    "            m.load_textual_moc(file_path)\n",
    "            if getattr(m, \"moc\", None) is None:\n",
    "                print(f\"[WARN] {os.path.basename(file_path)} has no spatial MOC â skipped.\")\n",
    "                continue\n",
    "            metadata.append(m)\n",
    "            filenames.append(os.path.basename(file_path))\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] loading {file_path}: {e}\")\n",
    "\n",
    "    if not metadata:\n",
    "        raise ValueError(\"No usable MOCs found (all missing or invalid).\")\n",
    "    return metadata, filenames\n",
    "\n",
    "\n",
    "def get_doc_title(moc: object, filename: str) -> str:\n",
    "    moc_data = getattr(moc, \"moc_data\", {}) or {}\n",
    "    title = (\n",
    "        moc_data.get(\"title\")\n",
    "        or moc_data.get(\"name\")\n",
    "        or moc_data.get(\"target\")\n",
    "        or os.path.splitext(filename)[0]\n",
    "    )\n",
    "    return str(title)\n",
    "\n",
    "\n",
    "def build_contexts_with_meta(results: List[ResultTuple], max_chars: int = 1500) -> List[Tuple[int, str]]:\n",
    "    contexts: List[Tuple[int, str]] = []\n",
    "    for i, (moc, sim, fn) in enumerate(results, start=1):\n",
    "        moc_data = getattr(moc, \"moc_data\", {}) or {}\n",
    "        txt = (moc_data.get(\"text\", \"\") or \"\")[:max_chars]\n",
    "        title = get_doc_title(moc, fn)\n",
    "        contexts.append((i, f\"(file={fn}, title={title}, sim={sim:.3f})\\n{txt}\"))\n",
    "    return contexts\n",
    "\n",
    "\n",
    "def parse_used_doc_indices(answer_text: str) -> List[int]:\n",
    "    m = re.search(r\"USED:\\s*([0-9,\\s]*)\\s*$\", answer_text)\n",
    "    if not m:\n",
    "        return []\n",
    "    payload = m.group(1).strip()\n",
    "    if not payload:\n",
    "        return []\n",
    "    nums: List[int] = []\n",
    "    for tok in payload.split(\",\"):\n",
    "        tok = tok.strip()\n",
    "        if tok.isdigit():\n",
    "            nums.append(int(tok) - 1)\n",
    "    return nums\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Semantic retrieval\n",
    "# -----------------------------\n",
    "def process_mocs_for_embeddings(\n",
    "    moc_list: List[object],\n",
    "    model: SentenceTransformer,\n",
    ") -> Tuple[np.ndarray, List[object]]:\n",
    "    texts, kept = [], []\n",
    "    for m in moc_list:\n",
    "        txt = (getattr(m, \"moc_data\", {}) or {}).get(\"text\", \"\")\n",
    "        if txt:\n",
    "            texts.append(txt)\n",
    "            kept.append(m)\n",
    "    if not texts:\n",
    "        raise ValueError(\"No valid 'custom_text' found in any MOC for semantic retrieval.\")\n",
    "    emb = model.encode(texts, convert_to_numpy=True).astype(\"float32\")\n",
    "    faiss.normalize_L2(emb)\n",
    "    return emb, kept\n",
    "\n",
    "\n",
    "def build_faiss_index(embeddings: np.ndarray) -> faiss.IndexFlatIP:\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    idx = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    idx.add(embeddings)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def query_mocs_semantic(\n",
    "    query_text: str,\n",
    "    index: faiss.IndexFlatIP,\n",
    "    model: SentenceTransformer,\n",
    "    mocs_with_text: List[object],\n",
    "    filenames_with_text: List[str],\n",
    "    top_k: int,\n",
    "    similarity_threshold: float = SIMILARITY_THRESHOLD,\n",
    ") -> List[ResultTuple]:\n",
    "    if not query_text:\n",
    "        raise ValueError(\"Error: empty query.\")\n",
    "    if index.ntotal == 0:\n",
    "        raise ValueError(\"Error: FAISS index is empty.\")\n",
    "    q = model.encode([query_text], convert_to_numpy=True).astype(\"float32\")\n",
    "    faiss.normalize_L2(q)\n",
    "    k = min(top_k, len(mocs_with_text))\n",
    "    sims, idxs = index.search(q, k)\n",
    "    raw = [(mocs_with_text[i], float(s), filenames_with_text[i]) for s, i in zip(sims[0], idxs[0])]\n",
    "    if similarity_threshold <= 0:\n",
    "        return raw\n",
    "    return [(m, s, fn) for (m, s, fn) in raw if s >= similarity_threshold]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Spatial containment (Angle-based) with detailed debug\n",
    "# -----------------------------\n",
    "def check_contains_debug(moc_obj, ra_deg: float, dec_deg: float) -> bool:\n",
    "    \"\"\"\n",
    "    Use mocpy.MOC.contains_lonlat with astropy Angle arrays (as in mocpy docs).\n",
    "    Prints type/shape/value of the returned mask when DEBUG=True.\n",
    "    \"\"\"\n",
    "    from astropy.coordinates import Angle\n",
    "\n",
    "    m = getattr(moc_obj, \"moc\", None)\n",
    "    if m is None:\n",
    "        if DEBUG:\n",
    "            print(\"  [debug] object has no .moc â skip\")\n",
    "        return False\n",
    "\n",
    "    # Normalize RA to [0, 360) for robustness\n",
    "    lon = Angle([ra_deg % 360.0], unit=\"deg\")\n",
    "    lat = Angle([dec_deg], unit=\"deg\")\n",
    "\n",
    "    try:\n",
    "        mask = m.contains_lonlat(lon=lon, lat=lat)  # numpy array([True/False])\n",
    "        arr = np.asarray(mask)\n",
    "        if DEBUG:\n",
    "            print(f\"  [debug] contains_lonlat type={type(mask).__name__} shape={arr.shape} value={arr}\")\n",
    "        return bool(arr.ravel()[0])\n",
    "    except Exception as e:\n",
    "        if DEBUG:\n",
    "            print(\"  [debug] contains_lonlat raised:\", e)\n",
    "        return False\n",
    "\n",
    "\n",
    "def query_mocs_positional(\n",
    "    ra_deg: float,\n",
    "    dec_deg: float,\n",
    "    moc_list: List[object],\n",
    "    filenames: List[str],\n",
    "    top_k: Optional[int] = None,\n",
    ") -> List[ResultTuple]:\n",
    "    results: List[ResultTuple] = []\n",
    "    for moc, fn in zip(moc_list, filenames):\n",
    "        title = get_doc_title(moc, fn)\n",
    "        print(f\"- Testing {fn} | title='{title}'\")\n",
    "        inside = check_contains_debug(moc, ra_deg, dec_deg)\n",
    "        print(f\"  â contains? {inside}\")\n",
    "        if inside:\n",
    "            results.append((moc, 1.0, fn))\n",
    "    if top_k and len(results) > top_k:\n",
    "        results = results[:top_k]\n",
    "    return results\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Optional synthetic MOC (for sanity checks)\n",
    "# -----------------------------\n",
    "def build_synthetic_cone_moc(ra_deg: float, dec_deg: float, radius_arcmin: float = 15.0, max_norder: int = 10):\n",
    "    \"\"\"\n",
    "    Build a synthetic cone MOC centered on (ra, dec) with given radius, then wrap into a TextualMOC.\n",
    "    Requires mocpy + astropy.\n",
    "    \"\"\"\n",
    "    from mocpy import MOC\n",
    "    import astropy.units as u\n",
    "    from astropy.coordinates import SkyCoord\n",
    "\n",
    "    center = SkyCoord(ra_deg * u.deg, dec_deg * u.deg, frame=\"icrs\")\n",
    "    moc = MOC.from_cone(center, radius_arcmin * u.arcmin, max_norder=max_norder)\n",
    "    tm = TextualMOC(moc)\n",
    "    try:\n",
    "        md = tm.moc_data if isinstance(tm.moc_data, dict) else {}\n",
    "        md.update({\n",
    "            \"title\": f\"SYNTHETIC_CONE_{radius_arcmin:.1f}arcmin\",\n",
    "            \"text\": f\"Synthetic cone MOC centered at RA={ra_deg}, DEC={dec_deg}, R={radius_arcmin} arcmin.\",\n",
    "            \"image\": \"\"\n",
    "        })\n",
    "        tm.moc_data = md\n",
    "    except Exception:\n",
    "        pass\n",
    "    return tm, f\"SYNTHETIC_CONE_{radius_arcmin:.1f}arcmin.json\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LLMs\n",
    "# -----------------------------\n",
    "def generate_response_with_mistral(query: str, contexts: List[Tuple[int, str]]) -> str:\n",
    "    docs_block = \"\\n\\n\".join([f\"[Doc {i}]\\n{text}\" for i, text in contexts])\n",
    "    system_msg = SystemMessage(content=TEXTUAL_SYSTEM_PROMPT)\n",
    "    human_text = f\"Question:\\n{query}\\n\\nProvided documents:\\n{docs_block}\\n\\n{OUTPUT_RULES_SUFFIX}\"\n",
    "    human_msg = HumanMessage(content=human_text)\n",
    "    llm = ChatOllama(model=TEXT_MODEL_NAME, temperature=TEXT_TEMPERATURE)\n",
    "    result = llm.invoke([system_msg, human_msg])\n",
    "    return str(result.content).strip()\n",
    "\n",
    "\n",
    "def analyze_image_with_vision_llm(image_url: str, vision_llm: ChatOllama, task: str, context: str = \"\") -> str:\n",
    "    import requests, base64\n",
    "    if not image_url:\n",
    "        return \"No image URL provided.\"\n",
    "    try:\n",
    "        resp = requests.get(image_url, timeout=20)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.content\n",
    "        ctype = resp.headers.get(\"Content-Type\", \"\").lower()\n",
    "        if not ctype.startswith(\"image/\"):\n",
    "            ctype = \"image/jpeg\"\n",
    "        encoded = base64.b64encode(data).decode(\"utf-8\")\n",
    "        data_uri = f\"data:{ctype};base64,{encoded}\"\n",
    "    except Exception as e:\n",
    "        return f\"Failed to fetch image: {e}\"\n",
    "\n",
    "    prompt_text = IMAGE_ANALYZER_PROMPT.format(task=task, context=context or \"N/A\")\n",
    "    system_msg = SystemMessage(\n",
    "        content=\"You are an astrophysics assistant analyzing images. \"\n",
    "                \"Be concise and cautious: if unsure, answer 'I don't know'.\"\n",
    "    )\n",
    "    human_msg = HumanMessage(\n",
    "        content=[{\"type\": \"text\", \"text\": prompt_text},\n",
    "                 {\"type\": \"image_url\", \"image_url\": {\"url\": data_uri}}]\n",
    "    )\n",
    "    try:\n",
    "        res = vision_llm.invoke([system_msg, human_msg])\n",
    "        return str(res.content).strip()\n",
    "    except Exception as e:\n",
    "        return f\"Image analysis error: {e}\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Visualization\n",
    "# -----------------------------\n",
    "def visualize_many_in_one_aladin(selected: List[ResultTuple], survey: str = DEFAULT_SURVEY, fov: int = DEFAULT_FOV):\n",
    "    try:\n",
    "        from ipyaladin import Aladin\n",
    "        from ipywidgets import VBox, HTML\n",
    "        from IPython.display import display\n",
    "    except Exception as exc:\n",
    "        print(\"ipyaladin/ipywidgets unavailable (skipping visualization):\", exc)\n",
    "        return None\n",
    "\n",
    "    aladin = Aladin(target=\"Sgr A*\", fov=fov, survey=survey)\n",
    "    display(aladin)\n",
    "\n",
    "    rows = []\n",
    "    for (moc, sim, fn) in selected:\n",
    "        title = get_doc_title(moc, fn)\n",
    "        moc_data = getattr(moc, \"moc_data\", {}) or {}\n",
    "        image_url = moc_data.get(\"image\", \"\") or \"\"\n",
    "        try:\n",
    "            aladin.add_moc(getattr(moc, \"moc\"), name=f\"{title} | {fn} | Sim: {sim:.4f}\")\n",
    "        except Exception as exc:\n",
    "            print(f\"Could not add MOC for {fn}: {exc}\")\n",
    "        if image_url:\n",
    "            rows.append(HTML(\n",
    "                value=f\"File: <b>{fn}</b> | Title: <b>{title}</b> | Similarity: <b>{sim:.4f}</b> | \"\n",
    "                      f\"Image: <a href='{image_url}' target='_blank'>{image_url}</a>\"\n",
    "            ))\n",
    "        else:\n",
    "            rows.append(HTML(\n",
    "                value=f\"File: <b>{fn}</b> | Title: <b>{title}</b> | Similarity: <b>{sim:.4f}</b> | Image: N/A\"\n",
    "            ))\n",
    "    if rows:\n",
    "        display(VBox(rows))\n",
    "    return aladin\n",
    "\n",
    "\n",
    "def analyze_and_visualize_used(results: List[ResultTuple], used_indices: List[int],\n",
    "                               vision_llm: ChatOllama, task: str, context: str = \"\",\n",
    "                               survey: str = DEFAULT_SURVEY, fov: int = DEFAULT_FOV):\n",
    "    if not used_indices:\n",
    "        print(\"No relevant documents: the selection is empty.\")\n",
    "        return\n",
    "    selected = []\n",
    "    for idx in used_indices:\n",
    "        if 0 <= idx < len(results):\n",
    "            selected.append(results[idx])\n",
    "    if not selected:\n",
    "        print(\"No relevant documents: the selection is empty.\")\n",
    "        return\n",
    "\n",
    "    visualize_many_in_one_aladin(selected, survey=survey, fov=fov)\n",
    "\n",
    "    BOLD_CYAN = \"\\033[1;36m\"\n",
    "    BOLD_MAGENTA = \"\\033[1;35m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "    for (moc, sim, fn) in selected:\n",
    "        title = get_doc_title(moc, fn)\n",
    "        moc_data = getattr(moc, \"moc_data\", {}) or {}\n",
    "        image_url = moc_data.get(\"image\", \"\") or \"\"\n",
    "        print(f\"{BOLD_CYAN}=== VISION MODEL INPUT ==={RESET}\")\n",
    "        print(\n",
    "            f\"File: {fn}\\nTitle: {title}\\nSimilarity: {sim:.4f}\\nTask: {task}\\n\"\n",
    "            f\"Context: {context or 'N/A'}\\nImage URL: {image_url or 'N/A'}\"\n",
    "        )\n",
    "        obs = analyze_image_with_vision_llm(image_url, vision_llm, task, context) if image_url else \"No image URL available.\"\n",
    "        print(f\"{BOLD_MAGENTA}=== VISION MODEL OBSERVATIONS ==={RESET}\")\n",
    "        print(obs)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CLI: choose mode\n",
    "# -----------------------------\n",
    "def ask_user_mode_and_inputs() -> Tuple[str, Optional[Tuple[float, float]], Optional[str]]:\n",
    "    print(\"\\nChoose mode:\")\n",
    "    print(\"  [pos] Positional â enter RA/DEC (deg) to select MOCs that contain the position\")\n",
    "    print(\"  [sem] Semantic   â enter a natural-language query\")\n",
    "    mode = input(\"Mode [pos/sem] (default=pos): \").strip().lower() or \"pos\"\n",
    "    if mode == \"sem\":\n",
    "        q = input(\"Enter your textual query:\\n> \").strip() or \"Summarize known properties of the field.\"\n",
    "        return \"sem\", None, q\n",
    "\n",
    "    print(\"\\nEnter coordinates in decimal degrees.\")\n",
    "    ra_s = input(\"RA (deg) [default=187.70593041666663]: \").strip()\n",
    "    dec_s = input(\"DEC (deg) [default=12.39]: \").strip()\n",
    "    try:\n",
    "        ra = float(ra_s) if ra_s else 187.70593041666663\n",
    "        dec = float(dec_s) if dec_s else 12.39\n",
    "    except ValueError:\n",
    "        print(\"Invalid numbers; using defaults.\")\n",
    "        ra = 187.70593041666663\n",
    "        dec = 12.39\n",
    "    return \"pos\", (ra, dec), None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    directory = os.environ.get(\"MOC_DIR\", \"moc_gals\")\n",
    "    top_k = 5  # positional: we can keep many; semantic: FAISS k\n",
    "\n",
    "    print_runtime_config(directory)\n",
    "\n",
    "    mode, ra_dec, query_text = ask_user_mode_and_inputs()\n",
    "\n",
    "    files = read_mocs_from_directory(directory)\n",
    "    if not files:\n",
    "        print(\"No MOC files found.\")\n",
    "        return\n",
    "    print(f\"Found {len(files)} MOC files in: {directory}\")\n",
    "\n",
    "    metadata, filenames = load_all_mocs(files)\n",
    "\n",
    "    # Optionally add a synthetic MOC centered at the chosen coords (for sanity test)\n",
    "    if ADD_SYNTHETIC_MOC and mode == \"pos\" and ra_dec is not None:\n",
    "        try:\n",
    "            syn_moc, syn_name = build_synthetic_cone_moc(ra_dec[0], ra_dec[1], radius_arcmin=20.0, max_norder=10)\n",
    "            metadata.append(syn_moc)\n",
    "            filenames.append(syn_name)\n",
    "            print(f\"[Synthetic] appended {syn_name} (20 arcmin cone) for debug.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Synthetic] could not build synthetic MOC: {e}\")\n",
    "\n",
    "    vision_llm = ChatOllama(model=VISION_MODEL_NAME, temperature=VISION_TEMPERATURE)\n",
    "\n",
    "    if mode == \"pos\":\n",
    "        ra_deg, dec_deg = ra_dec if ra_dec else (187.70593041666663, 12.39)\n",
    "        print(f\"\\n[Positional] Using RA={ra_deg} deg, DEC={dec_deg} deg\")\n",
    "\n",
    "        print(\"\\n[Positional] Running containment checks per MOC (Angle-based)...\")\n",
    "        results = query_mocs_positional(ra_deg, dec_deg, metadata, filenames, top_k=top_k)\n",
    "\n",
    "        if not results:\n",
    "            print(\"\\n[DIAGNOSTIC] No MOCs reported containment. Dumping raw checks again for ALL MOCs:\")\n",
    "            for moc, fn in zip(metadata, filenames):\n",
    "                title = get_doc_title(moc, fn)\n",
    "                print(f\"* {fn} | title='{title}'\")\n",
    "                _ = check_contains_debug(moc, ra_deg, dec_deg)  # prints internal return types/values\n",
    "            print(\"\\nConclusion: none returned True. Please verify that each JSON really contains a valid IVOA MOC \"\n",
    "                  \"(TextualMOC.load_textual_moc must populate .moc) and that coordinates are ICRS in degrees.\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nPositional retrieval: k={len(results)}; (sim=1.000 for all)\")\n",
    "\n",
    "        contexts = build_contexts_with_meta(results, max_chars=1500)\n",
    "        text_query = (f\"Summarize what these documents state about RA={ra_deg:.6f} deg, \"\n",
    "                      f\"DEC={dec_deg:.6f} deg. Include inline citations like [Doc n].\")\n",
    "\n",
    "        rag_text = generate_response_with_mistral(text_query, contexts)\n",
    "        used_zero_based = parse_used_doc_indices(rag_text)\n",
    "        if not used_zero_based:\n",
    "            print(\"LLM did not cite any source (empty USED) â falling back to all selected docs.\")\n",
    "            used_zero_based = list(range(len(results)))\n",
    "\n",
    "        print(\"\\n=== RAG-BASED TEXTUAL RESPONSE (Positional) ===\")\n",
    "        print(rag_text)\n",
    "        print(\"USED indices    : \" + \", \".join(str(i + 1) for i in used_zero_based))\n",
    "\n",
    "        analyze_and_visualize_used(\n",
    "            results=results,\n",
    "            used_indices=used_zero_based,\n",
    "            vision_llm=vision_llm,\n",
    "            task=VISION_TASK_DEFAULT,\n",
    "            context=VISION_CONTEXT_DEFAULT,\n",
    "            survey=DEFAULT_SURVEY,\n",
    "            fov=DEFAULT_FOV,\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # SEMANTIC mode\n",
    "    assert query_text is not None\n",
    "    print(f\"\\n[Semantic] Query: {query_text}\")\n",
    "\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    embeddings, mocs_with_text = process_mocs_for_embeddings(metadata, model)\n",
    "\n",
    "    filenames_with_text = []\n",
    "    for m in mocs_with_text:\n",
    "        idx = metadata.index(m)\n",
    "        filenames_with_text.append(filenames[idx])\n",
    "\n",
    "    index = build_faiss_index(embeddings)\n",
    "\n",
    "    results = query_mocs_semantic(\n",
    "        query_text,\n",
    "        index,\n",
    "        model,\n",
    "        mocs_with_text,\n",
    "        filenames_with_text,\n",
    "        top_k=top_k,\n",
    "        similarity_threshold=SIMILARITY_THRESHOLD,\n",
    "    )\n",
    "\n",
    "    if not results:\n",
    "        print(f\"No retrievals passed the similarity threshold (>= {SIMILARITY_THRESHOLD}).\")\n",
    "        return\n",
    "\n",
    "    print(\"Top-k (filtered) : k={}; sims=[{}]\".format(len(results), \", \".join(f\"{s:.3f}\" for _, s, _ in results)))\n",
    "\n",
    "    contexts = build_contexts_with_meta(results, max_chars=1500)\n",
    "    rag_text = generate_response_with_mistral(query_text, contexts)\n",
    "    used_zero_based = parse_used_doc_indices(rag_text)\n",
    "    if not used_zero_based:\n",
    "        print(\"No relevant documents: the LLM did not cite any source (empty USED).\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== RAG-BASED TEXTUAL RESPONSE ===\")\n",
    "    print(rag_text)\n",
    "    print(\"USED indices    : \" + \", \".join(str(i + 1) for i in used_zero_based))\n",
    "\n",
    "    analyze_and_visualize_used(\n",
    "        results=results,\n",
    "        used_indices=used_zero_based,\n",
    "        vision_llm=vision_llm,\n",
    "        task=VISION_TASK_DEFAULT,\n",
    "        context=VISION_CONTEXT_DEFAULT,\n",
    "        survey=DEFAULT_SURVEY,\n",
    "        fov=DEFAULT_FOV,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4233f1d9-bb0e-4b76-90bc-636b055c8fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1de95f6-48c1-43d0-b60d-9c2ab87ccd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73257303-effe-48a6-8f02-f9a31c951cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402be7b-68b1-4f36-a260-97f84307f981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6160d1-3313-4953-8181-4e98021de6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a326037-8681-4b3f-8823-4fe7f05300ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a6d5f-91c9-4d41-bc6a-f4883e3dbc47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aimoc] *",
   "language": "python",
   "name": "conda-env-aimoc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
